{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e4c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668315f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^가-힣0-9]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 한국어 불용어 리스트 - 년, 또, 그런, 좀, 잘, 개, 아니, 씨, 안, 다시, 못하, 문제, 사람, 때문\n",
    "    stopwords = [\n",
    "        '이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '주', \n",
    "        '등', '같', '우리', '때', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하', \n",
    "        '그것', '두', '말하', '알', '그러나', '받', '일', '더', '사회', \n",
    "        '많', '그리고', '좋', '크', '따르', '중', '나오', '가지', '시키', '만들', '지금', '생각하', \n",
    "        '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', '어떤', '내', '경우',\n",
    "        '명', '생각', '시간', '그녀', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자',\n",
    "        '전', '들', '사실', '이렇', '점', '싶', '말', '정도', '원', '통하', '소리', '놓'\n",
    "    ]\n",
    "    \n",
    "    # 불용어 제거\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ade358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation column names for each file based on the earlier inspection\n",
    "conversation_columns = {\n",
    "    \"./data/train_sr_cleaned.csv\": \"conversation_sr_cleaned\",\n",
    "    \"./data/train_augmented_wv_.csv\": \"conversation\",\n",
    "    \"./data/LLaMa2_Augmentation_trian.csv\": \"conversation\",\n",
    "    \"./data/train.csv\": \"conversation\",\n",
    "    \"./data/dominant_tone_data_transformers.csv\": \"conversation\",\n",
    "    \"./data/dominant_tone_data_transformers.csv\": \"conversation\"\n",
    "}\n",
    "\n",
    "# Load and preprocess the data from each file, then concatenate them\n",
    "all_dataframes = []\n",
    "\n",
    "for file_path, conv_column in conversation_columns.items():\n",
    "    df_temp = pd.read_csv(file_path)\n",
    "    df_temp = df_temp[['class', conv_column]]\n",
    "    df_temp.columns = ['class', 'conversation']  # Renaming columns for uniformity\n",
    "    df_temp['conversation'] = df_temp['conversation'].apply(clean_text)\n",
    "    all_dataframes.append(df_temp)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "merged_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "test = pd.read_json('./data/test.json').transpose()\n",
    "\n",
    "# train 데이터의 텍스트 열 정규화\n",
    "merged_data['conversation'] = merged_data['conversation'].apply(clean_text)\n",
    "test['conversation'] = test['text'].apply(clean_text)\n",
    "\n",
    "# 지정된 클래스를 숫자로 인코딩\n",
    "label_dict = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3\n",
    "}\n",
    "merged_data['label_encoded'] = merged_data['class'].map(label_dict)\n",
    "\n",
    "merged_data.drop_duplicates(subset=['conversation'], inplace=True)\n",
    "\n",
    "\n",
    "# NaN 값을 가진 행 제거\n",
    "merged_data.dropna(subset=['conversation', 'label_encoded'], inplace=True)\n",
    "\n",
    "# 빈 문자열 값을 가진 행 제거\n",
    "merged_data = merged_data[merged_data['conversation'] != \"\"]\n",
    "\n",
    "merged_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ab78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab 토큰화\n",
    "mecab = Mecab()\n",
    "tokenizer = lambda text: mecab.morphs(text)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer, max_features=6000)\n",
    "all_data_tfidf = tfidf_vectorizer.fit_transform(merged_data['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57682958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models_params = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(solver='liblinear', max_iter=1500),\n",
    "        'params': {\n",
    "            'C': [10, 100],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.05],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {\n",
    "            'C': [ 1, 100],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7692c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Model 1/5: RandomForest\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=10; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=10; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=10; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=10; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=10; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=  16.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=  16.4s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=  16.4s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=  16.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=  16.3s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=10; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=10; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=10; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=10; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=10; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=  15.2s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=  15.2s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=  15.1s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=  15.1s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=  15.0s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=10; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=10; total time=   0.2s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=10; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "\n",
      "Model: RandomForest with {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation Accuracy: 0.9223\n",
      "Validation F1 Score: 0.9226\n",
      "Test Accuracy: 0.8550\n",
      "\n",
      "Processing Model 2/5: LogisticRegression\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   2.2s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   2.2s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   2.0s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   2.3s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   2.1s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   1.0s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   1.0s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.9s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.9s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.9s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   2.4s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   2.4s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   2.4s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   2.3s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   1.4s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   1.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   1.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   1.6s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   1.5s\n",
      "\n",
      "Model: LogisticRegression with {'C': 100, 'penalty': 'l2'}\n",
      "Validation Accuracy: 0.9282\n",
      "Validation F1 Score: 0.9282\n",
      "Test Accuracy: 0.8575\n",
      "\n",
      "Processing Model 3/5: GradientBoosting\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.01, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ...learning_rate=0.01, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ...learning_rate=0.05, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.05, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.05, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.05, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ...learning_rate=0.05, max_depth=3, n_estimators=50; total time= 1.0min\n",
      "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ..learning_rate=0.05, max_depth=3, n_estimators=100; total time= 2.0min\n",
      "[CV] END ...learning_rate=0.05, max_depth=5, n_estimators=50; total time= 1.8min\n",
      "[CV] END ...learning_rate=0.05, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ...learning_rate=0.05, max_depth=5, n_estimators=50; total time= 1.8min\n",
      "[CV] END ...learning_rate=0.05, max_depth=5, n_estimators=50; total time= 1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...learning_rate=0.05, max_depth=5, n_estimators=50; total time= 1.7min\n",
      "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "[CV] END ..learning_rate=0.05, max_depth=5, n_estimators=100; total time= 3.5min\n",
      "\n",
      "Model: GradientBoosting with {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}\n",
      "Validation Accuracy: 0.8503\n",
      "Validation F1 Score: 0.8520\n",
      "Test Accuracy: 0.8200\n",
      "\n",
      "Processing Model 4/5: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 5.9min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 5.9min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 6.0min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 5.9min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 5.9min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 9.3min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 9.3min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 9.3min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 9.3min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 9.3min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 5.9min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 5.9min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 5.9min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 5.9min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 5.9min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=17.4min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=17.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=17.2min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=17.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=17.5min\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time= 4.4min\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time= 4.4min\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time= 4.4min\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time= 4.3min\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time= 4.3min\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=10.8min\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=10.7min\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=11.0min\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=10.9min\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=11.0min\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time= 4.4min\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time= 4.4min\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time= 4.4min\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time= 4.4min\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time= 4.4min\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=14.3min\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=14.2min\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=14.3min\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=14.2min\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=14.2min\n",
      "\n",
      "Model: SVM with {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Validation Accuracy: 0.9309\n",
      "Validation F1 Score: 0.9311\n",
      "Test Accuracy: 0.8725\n",
      "\n",
      "Processing Model 5/5: KNN\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=   3.7s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=   3.4s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=   3.4s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=   3.3s\n",
      "[CV] END ...metric=euclidean, n_neighbors=3, weights=uniform; total time=   3.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=   3.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=   3.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=   3.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=   3.3s\n",
      "[CV] END ..metric=euclidean, n_neighbors=3, weights=distance; total time=   3.2s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=   3.5s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=   3.6s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=   3.6s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=   3.6s\n",
      "[CV] END ...metric=euclidean, n_neighbors=5, weights=uniform; total time=   3.6s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=   3.4s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=   3.5s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=   3.5s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=   3.5s\n",
      "[CV] END ..metric=euclidean, n_neighbors=5, weights=distance; total time=   3.6s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time=  19.0s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time=  19.0s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time=  19.0s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time=  19.5s\n",
      "[CV] END ...metric=manhattan, n_neighbors=3, weights=uniform; total time=  19.2s\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time=  19.0s\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time=  19.0s\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time=  18.9s\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time=  19.0s\n",
      "[CV] END ..metric=manhattan, n_neighbors=3, weights=distance; total time=  19.2s\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time=  19.5s\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time=  19.5s\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time=  19.5s\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time=  18.8s\n",
      "[CV] END ...metric=manhattan, n_neighbors=5, weights=uniform; total time=  19.8s\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time=  19.2s\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time=  19.3s\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time=  18.8s\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time=  19.8s\n",
      "[CV] END ..metric=manhattan, n_neighbors=5, weights=distance; total time=  19.3s\n",
      "\n",
      "Model: KNN with {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Validation Accuracy: 0.8959\n",
      "Validation F1 Score: 0.8960\n",
      "Test Accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "X = all_data_tfidf  \n",
    "y = merged_data['label_encoded'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "test_tfidf = tfidf_vectorizer.transform(test['conversation'])\n",
    "answer_df = pd.read_csv(\"./data/answer.csv\")\n",
    "\n",
    "total_models = len(models_params)\n",
    "for idx, (model_name, mp) in enumerate(models_params.items(), 1):\n",
    "    print(f\"\\nProcessing Model {idx}/{total_models}: {model_name}\")\n",
    "    \n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False, verbose=2, scoring='f1_macro')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_val, y_pred)\n",
    "    val_f1_score = f1_score(y_val, y_pred, average='macro')  # Calculate F1 score\n",
    "    \n",
    "    class_report = classification_report(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nModel: {model_name} with {clf.best_params_}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1_score:.4f}\")  # Print F1 Score\n",
    "    \n",
    "    results[(model_name, 'validation', str(clf.best_params_))] = {\n",
    "        'accuracy': val_accuracy,\n",
    "        'f1_score': val_f1_score,\n",
    "        'report': class_report,\n",
    "        'best_params': clf.best_params_\n",
    "    }\n",
    "    \n",
    "    # test.csv에 대한 예측 수행\n",
    "    y_pred_test = best_model.predict(test_tfidf)\n",
    "    \n",
    "    # 예측 결과와 answer.csv의 정답을 비교하여 정확도 계산\n",
    "    test_accuracy = accuracy_score(answer_df['class'], y_pred_test)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    results[(model_name, 'test', str(clf.best_params_))] = {\n",
    "        'accuracy': test_accuracy,\n",
    "        'best_params': clf.best_params_\n",
    "    }\n",
    "    \n",
    "    # Save the model if accuracy is above threshold\n",
    "    if test_accuracy >= 0.83:\n",
    "        params_str = \"_\".join([f\"{k}={v}\" for k, v in clf.best_params_.items()])\n",
    "        model_save_path = f'{model_name}_{params_str}_{test_accuracy}.pkl'\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b61ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edab6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
